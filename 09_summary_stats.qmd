---
title: '9: Summary statistics'
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# Import Pandas

import pandas as pd
import io
import boto3
```

```{python}
# Import data files from S3 bucket into dataframe

s3c = boto3.client('s3', region_name="us-east-2",aws_access_key_id="AKIAQ3EGRIQMCS55ACH2",aws_secret_access_key="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")
obj = s3c.get_object(Bucket="yelpdataset14",Key="yelp_data.csv")
```

```{python}
df_data = pd.read_csv(io.BytesIO(obj["Body"].read()))
df_data.head()
```

```{python}
# Drop null values

df_data.dropna(inplace=True)
```

```{python}
import pandas as pd

# Test to check if all columns are present
def test_columns_present(df_data):
    try:
        expected_columns = [
            'review_id', 'business_id', 'stars_x', 'date', 'text', 'useful',
            'funny', 'cool', 'day', 'month', 'year', 'time', 'total_votes',
            'state', 'stars_y', 'longitude', 'is_open', 'address', 'latitude',
            'name', 'categories', 'postal_code', 'city', 'review_count',
            'attribute_count', 'category_count'
        ]
        assert all(col in df_data.columns for col in expected_columns), f"Missing columns: {set(expected_columns) - set(df.columns)}"
    except Exception as e:
        raise AssertionError(f"Error occurred while testing columns presence: {e}")

# Test to check if specific columns have correct data types
def test_column_data_types(df_data):
    try:
        expected_data_types = {
            'review_id': object, 'business_id': object , 'stars_x': float, 'date': object,
            'text': object, 'useful': float, 'funny': float, 'cool': float,
            'day': float, 'month': float, 'year': float, 'time': object,
            'total_votes': float, 'state': object, 'stars_y': float,
            'longitude': float, 'is_open': float, 'address': object,
            'latitude': float, 'name': object, 'categories': object,
            'postal_code': float, 'city': object, 'review_count': float,
            'attribute_count': float, 'category_count': float
        }
        for col, dtype in expected_data_types.items():
            assert df_data[col].dtype == dtype, f"Column '{col}' has incorrect data type: Expected {dtype}, Found {df_data[col].dtype}"
    except Exception as e:
        raise AssertionError(f"Error occurred while testing column data types: {e}")

# Test to check if there are any missing values in any column
def test_missing_values(df_data):
    try:
        assert not df_data.isnull().values.any(), "DataFrame contains missing values"
    except Exception as e:
        raise AssertionError(f"Error occurred while testing missing values: {e}")

# Test to check if certain columns contain only unique values
def test_unique_values(df_data):
    try:
        columns_to_check_uniqueness = ['review_id', 'business_id']
        for col in columns_to_check_uniqueness:
            assert df_data[col].nunique() == len(df[col]), f"Column '{col}' does not contain unique values"
    except Exception as e:
        raise AssertionError(f"Error occurred while testing unique values: {e}")
        
# Test to ensure that only rows with states 'PA', 'TN', 'FL', and 'IN' are present       
def test_valid_states(df_data):
    try:
        valid_states = ['PA', 'TN', 'FL', 'IN']
        assert all(df_data['state'].isin(valid_states)), "Incorrect states in the dataframe"
    except AssertionError as e:
        raise AssertionError(f"Error occurred while testing valid states: {e}")

# Run the tests
if __name__ == "__main__":
    try:
        test_columns_present(df_data)
        test_column_data_types(df_data)
        test_missing_values(df_data)
        test_missing_values(df_data)
        test_valid_states(df_data)
        print("All tests passed successfully!")
    except AssertionError as e:
        print(f"Test failed: {e}")
```

```{python}
s3c = boto3.client('s3', region_name="us-east-2",aws_access_key_id="AKIAQ3EGRIQMCS55ACH2",aws_secret_access_key="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")
obj = s3c.get_object(Bucket="yelpdataset14",Key="fourStates_review.csv")
```

```{python}
df_reviews = pd.read_csv(io.BytesIO(obj["Body"].read()))
df_reviews.head()
```

```{python}
# Drop null values

df_reviews.dropna(inplace=True)
```

```{python}
import pandas as pd

# Test to check if all columns are present
def test_columns_present_reviews(df_reviews):
    try:
        expected_columns = ['review_id', 'business_id', 'user_id', 'stars', 'date', 'text', 'useful', 'funny', 'cool']
        assert all(col in df_reviews.columns for col in expected_columns), f"Missing columns: {set(expected_columns) - set(df_reviews.columns)}"
    except AssertionError as e:
        raise AssertionError(f"Error occurred while testing columns presence: {e}")

# Test to check if specific columns have correct data types
def test_column_data_types_reviews(df_reviews):
    try:
        expected_data_types = {
            'review_id': object, 'business_id': object, 'user_id': object, 'stars': float, 'date': object,
            'text': object, 'useful': float, 'funny': float, 'cool': float
        }
        for col, dtype in expected_data_types.items():
            assert df_reviews[col].dtype == dtype, f"Column '{col}' has incorrect data type: Expected {dtype}, Found {df_reviews[col].dtype}"
    except AssertionError as e:
        raise AssertionError(f"Error occurred while testing column data types: {e}")

# Test to check if there are any missing values in any column
def test_missing_values_reviews(df_reviews):
    try:
        missing_columns = df_reviews.columns[df_reviews.isnull().any()].tolist()
        assert not df_reviews.isnull().values.any(), f"DataFrame contains missing values in columns: {missing_columns}"
    except AssertionError as e:
        raise AssertionError(f"Error occurred while testing missing values: {e}")

# Test to check if certain columns contain only unique values
def test_unique_values_reviews(df_reviews):
    try:
        columns_to_check_uniqueness = ['review_id']
        for col in columns_to_check_uniqueness:
            assert df_reviews[col].nunique() == len(df_reviews[col]), f"Column '{col}' does not contain unique values"
    except AssertionError as e:
        raise AssertionError(f"Error occurred while testing unique values: {e}")

# Run the tests
if __name__ == "__main__":
    try:
        test_columns_present_reviews(df_reviews)
        test_column_data_types_reviews(df_reviews)
        test_missing_values_reviews(df_reviews)
        test_unique_values_reviews(df_reviews)
        print("All tests passed successfully!")
    except AssertionError as e:
        print(f"Test failed: {e}")
```

```{python}
# Check DataTypes of the columns in dataframe
df_data.info()
```

```{python}
# Calculate and show summary statistics of the numerical columns

df_data.describe()
```

```{python}
# Check DataTypes of the columns in dataframe

df_reviews.info()
```

```{python}
# Calculate and show summary statistics of the numerical columns

df_reviews.describe()
```

```{python}
# Check the count of unique cities and states in the dataset

unique_cities = df_data['city'].nunique()
unique_states = df_data['state'].nunique()
print(f"Distinct city count: {unique_cities}")
print(f"Distinct state count: {unique_states}")

# We obeserved that the data we are using has details of businesses regsitered with Yelp in 584 cities of 4 different states
```

```{python}
# Pull up the statistics of mean stars for review , review count , number of cities in which a store is present grouped 
# by the name of the store

business_rating=df_data.groupby(['name']).stars_x.mean()
business_review_count=df_data.groupby(['name']).review_count.sum()
business_city_name=df_data.groupby(['name']).city.nunique()

result = pd.concat([business_rating,business_review_count,business_city_name], axis=1 , join='inner')
result.sort_values(by=['city'], ascending=False).head(10)
```

We pulled up the statistic for 10 businesses , their average star rating , total review count and the count of cities in
which the business is present and found that the average ratings of popular businesses is also not good.

```{python}
# Top 10 business with more than 1000 reviews greater than 4 sorted by count of cities in which the business is expanded

result[(result['stars_x'] >=4) & (result['review_count'] >= 1000)].sort_values(by=['city'], ascending=False).head(10)
```

Next we pulled up the average ratings , review count and city with 1000 reviews with greater than 4 star ratings and count of cities in which their business is expanded and found the above stores in ascending order of the cities in which the business is expanded

```{python}
# Number of business registered with Yelp in each city 
df_data.city.value_counts()
```

We observed that maximum businesses registered with Yelp are from top cities like Philadelphia,Tampa,Nashiville and Indianapolis

```{python}
# Top 50 business with highest number of reviews with the city and star review
df_data[['name', 'review_count', 'city', 'stars_x']].sort_values(ascending=False, by="review_count")[0:50]
```

```{python}
# Top  50 cities by business listed
city_business_counts =df_data[['city', 'business_id']].groupby(['city'])\
['business_id'].agg('count').sort_values(ascending=False)
```

```{python}
city_business_counts = pd.DataFrame(data=city_business_counts)
```

```{python}
city_business_counts.rename(columns={'business_id' : 'number_of_businesses'}, inplace=True)
```

```{python}
#Plot a graph with the above extracted data for 50 cities
import matplotlib.pyplot as plt
import seaborn as sns 
import matplotlib.gridspec as gridspec 
import matplotlib.gridspec as gridspec

city_business_counts[0:50].sort_values(ascending=False, by="number_of_businesses")\
.plot(kind='barh', stacked=False, figsize=[10,10], colormap='winter')
plt.title('Top 50 cities by businesses listed')
```

We also plotted a graph for top 50 cities whose businesses are registered with Yelp . It means that Yelp is quite popular in these cities.

```{python}
#Plotting a graph for star rating distribution overall
x=df_data['stars_x'].value_counts()
x=x.sort_index()
#plot
plt.figure(figsize=(8,4))
ax= sns.barplot(x=x.index, y=x.values, alpha=0.8)
plt.title("Star Rating Distribution")
plt.ylabel('# of businesses', fontsize=12)
plt.xlabel('Star Ratings ', fontsize=12)

#adding the text labels
rects = ax.patches
labels = x.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()
```

As we can see from the plot above, majority of businesses have a 5-star rating.

```{python}
#Count of stars for each category of stars (1,2,3,4,5)
df_data['stars_x'].value_counts()
```

```{python}
#Top 10 users in Yelp
user_agg=df_reviews.groupby('user_id').agg({'review_id':['count'],'date':['min','max'],
                                'useful':['sum'],'funny':['sum'],'cool':['sum'],
                               'stars':['mean']})
```

```{python}
user_agg=user_agg.sort_values([('review_id','count')],ascending=False)
print("          Top 10 Users in Yelp")
user_agg.head(10)
```

We also extracted the details of top 10 users in Yelp based on their review count and the sum of their useful , funny and cool reviews

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df_data['date'] = pd.to_datetime(df_data['date'])

# Create plots
plt.figure(figsize=(14, 8))

# Plot 1: Number of reviews over time
plt.subplot(2, 1, 1)
df_data.groupby('date').size().plot()
plt.title('Number of Reviews Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Reviews')

# Plot 2: Average review score over time
plt.subplot(2, 1, 2)
df_data.groupby('date')['stars_x'].mean().plot()
plt.title('Average Review Score Over Time')
plt.xlabel('Date')
plt.ylabel('Average Review Score')

plt.tight_layout()
plt.show()
```

We observed that the number of reviews has increased continuously after 2010 . We also observed the maximum reviews posted during years 2019-2020 and specially during 2020 ( could be due to Covid) . The reviews were decreared to some extent after 2020 but are same as before 2020 now . So, there was a seasonal rise in count of reviews during Covid (2020).

We also observed that the average review score is quite same from 2008 to 2022 . 

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Group by 'state' column and count unique businesses
business_count_by_state = df_data.groupby('state')['business_id'].nunique()

# Plotting
plt.figure(figsize=(12, 6))
business_count_by_state.plot(kind='bar')
plt.title('Number of Businesses in Each State')
plt.xlabel('State')
plt.ylabel('Number of Businesses')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

We have the data from 4 states , we plotted a bar graph for number of businesses registered wit Yelp and observed that maximum businesses are registered from Pennsylvania and Florida.

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Group by 'state' and 'stars_x' (rating) columns and count reviews
ratings_by_state = df_data.groupby(['state', 'stars_x']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 6))

# Width of each bar group
bar_width = 0.2

# Positions for bars
positions = range(len(ratings_by_state.index))

# Colors for each star rating
colors = ['blue', 'green', 'orange', 'red', 'purple']

# Plotting each bar group for each star rating
for i, star_rating in enumerate(ratings_by_state.columns):
    plt.bar([p + i * bar_width for p in positions],
            ratings_by_state[star_rating],
            bar_width,
            color=colors[i],
            label=f'{star_rating} stars')

plt.title('Number of Ratings by State and Star Rating')
plt.xlabel('State')
plt.ylabel('Number of Ratings')
plt.xticks(positions, ratings_by_state.index, rotation=45)
plt.legend(title='Star Rating')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

From the above bar graph , we can see that maximum 5 star ratings are from Pennsylvania and Indiana.

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Group by 'state' and 'stars_x' (rating) columns and count reviews
ratings_by_state = df_data.groupby(['state', 'stars_x']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 6))
ratings_by_state.plot(kind='bar', stacked=True)
plt.title('Stacked Bar Plot of Ratings by State')
plt.xlabel('State')
plt.ylabel('Number of Ratings')
plt.xticks(rotation=45)
plt.legend(title='Star Rating')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Convert 'date' column to datetime
df_data['date'] = pd.to_datetime(df_data['date'])

# Extract month and year from the 'date' column
df_data['month'] = df_data['date'].dt.month
df_data['year'] = df_data['date'].dt.year

# Group by 'state', 'year', and 'month' and count reviews
ratings_by_state_month = df_data.groupby(['state', 'year', 'month']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 8))
sns.heatmap(ratings_by_state_month, cmap='YlGnBu')
plt.title('Heatmap of Ratings by State and Month')
plt.xlabel('Month')
plt.ylabel('State')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

The heatmap will visually represents the count of reviews by state and month, with different colors indicating different review counts. This visualization helps to identify patterns or trends in review counts across states and months.
